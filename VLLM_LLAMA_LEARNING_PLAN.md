# vLLM LLaMAæ¨ç†å­¦ä¹ è®¡åˆ’

## ç›®æ ‡
ä»¥LLaMAæ¨¡å‹ä¸ºåˆ‡å…¥ç‚¹ï¼Œç³»ç»Ÿå­¦ä¹ vLLMæ¨ç†æ¡†æ¶çš„æ ¸å¿ƒæŠ€æœ¯ï¼Œæœ€ç»ˆè¾¾åˆ°å¿«é€Ÿé€‚é…æ–°å¤§æ¨¡å‹çš„èƒ½åŠ›ã€‚

---

## ä¸€ã€ç¯å¢ƒé…ç½®å»ºè®®

### 1.1 æ¨èç¯å¢ƒæ–¹æ¡ˆï¼šCondaè™šæ‹Ÿç¯å¢ƒï¼ˆâœ… æ¨èï¼‰

**ç†ç”±ï¼š**
- âœ… ä¾¿äºæ–­ç‚¹è°ƒè¯•å’ŒIDEé›†æˆ
- âœ… ä¸å½±å“å®¿ä¸»æœºå…¶ä»–Pythonç¯å¢ƒ
- âœ… æ–¹ä¾¿åˆ‡æ¢ä¸åŒç‰ˆæœ¬çš„PyTorch/CUDA
- âœ… ä½ å·²ç»åœ¨å®¿ä¸»æœºç¯å¢ƒï¼Œä¸éœ€è¦é¢å¤–å®¹å™¨é…ç½®
- âœ… 4å¼ 4090Dæ˜¾å¡ç›´æ¥è®¿é—®ï¼Œæ— éœ€å®¹å™¨GPUæ˜ å°„

**ç¯å¢ƒæ­å»ºæ­¥éª¤ï¼š**

å‚è€ƒ [[https://docs.vllm.ai/en/latest/contributing/#license]]

```bash
# 1. It's recommended to use uv, a very fast Python environment manager, to create and manage Python environments.

## 1. install uv
curl -LsSf https://astral.sh/uv/install.sh | sh
source $HOME/.local/bin/env

## 2. create python3 env
uv venv --python 3.12 --seed
source .venv/bin/activate

## 3. I need to develop vLLM's Python and CUDA/C++ code,so:

## éœ€è¦å…ˆå®‰è£…ç³»ç»Ÿ Python å¼€å‘å¤´æ–‡ä»¶
sudo apt-get update
sudo apt-get install python3.12-dev build-essential

uv pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu129

## æœ‰äº›ä¾èµ–ï¼Œå¦‚nvidiaä¾èµ–ç›´æ¥ä»githubä¸Šä¸‹è½½ï¼Œå› æ­¤æœ€å¥½å¼€ä»£ç†
export http_proxy=http://192.168.11.6:7890
export https_proxy=http://192.168.11.6:7890
uv pip install -e . --no-build-isolation


```

### 1.2 IDEé…ç½®å»ºè®®

**VSCodeé…ç½®ï¼ˆæ¨èï¼‰ï¼š**

```json
// .vscode/launch.json - ç”¨äºæ–­ç‚¹è°ƒè¯•
{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python: vLLM Offline Inference",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/examples/offline_inference/basic/basic.py",
            "console": "integratedTerminal",
            "justMyCode": false,  // å…è®¸è°ƒè¯•vLLMå†…éƒ¨ä»£ç 
            "env": {
                "CUDA_VISIBLE_DEVICES": "0",  // å¼€å§‹æ—¶ç”¨å•å¡è°ƒè¯•
                "VLLM_LOGGING_LEVEL": "DEBUG"
            }
        },
        {
            "name": "Python: vLLM LLaMA Inference",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/my_debug_scripts/llama_debug.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "env": {
                "CUDA_VISIBLE_DEVICES": "0,1",
                "VLLM_LOGGING_LEVEL": "DEBUG"
            }
        }
    ]
}
```

---

## äºŒã€å­¦ä¹ è·¯çº¿ï¼ˆ3ä¸ªé˜¶æ®µï¼‰

### é˜¶æ®µ1ï¼šåŸºç¡€å…¥é—¨ - ç†è§£æ¨ç†æµç¨‹ï¼ˆ1-2å‘¨ï¼‰

#### ç›®æ ‡
æŒæ¡vLLMçš„åŸºæœ¬æ¶æ„å’ŒLLaMAæ¨¡å‹çš„æ¨ç†æµç¨‹ã€‚

#### 1.1 ç¬¬ä¸€ä¸ªå¯è¿è¡Œç¤ºä¾‹

**âš ï¸ æ¨¡å‹é€‰æ‹©è¯´æ˜ï¼š**

å­¦ä¹  vLLM æ¨ç†**å¼ºçƒˆå»ºè®®ä½¿ç”¨ LLaMA æ¶æ„çš„æ¨¡å‹**ï¼ŒåŸå› ï¼š
- vLLM æ ¸å¿ƒå®ç°ï¼ˆ`llama.py`ï¼‰æ˜¯æœ€æ ‡å‡†ã€æœ€å®Œæ•´çš„å‚è€ƒ
- PagedAttentionã€RoPEã€SwiGLU ç­‰æ ¸å¿ƒæŠ€æœ¯éƒ½ä»¥ LLaMA ä¸ºåŸå‹
- å…¶ä»–æ¨¡å‹éƒ½æ˜¯åœ¨ LLaMA åŸºç¡€ä¸Šä¿®æ”¹çš„ï¼Œå­¦ä¹ è·¯å¾„æœ€æ¸…æ™°

**æ¨èæ¨¡å‹ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š**

| æ¨¡å‹ | å¤§å° | ä¼˜åŠ¿ | æ˜¯å¦éœ€è¦æˆæƒ |
|------|------|------|--------------|
| **TinyLlama/TinyLlama-1.1B-Chat-v1.0** | 1.1GB | âœ… å®Œæ•´ LLaMA æ¶æ„<br>âœ… ä»£ç ç›´æ¥å¯¹åº” `llama.py`<br>âœ… å­¦ä¹ ä»·å€¼æœ€é«˜ | âŒ æ— éœ€ |
| meta-llama/Llama-3.2-1B-Instruct | 1.2GB | âœ… å®˜æ–¹ LLaMA<br>âš ï¸ éœ€è¦ç”³è¯·æƒé™ | âœ… éœ€è¦ |
| Qwen/Qwen2.5-0.5B-Instruct | 1GB | âœ… æ¶æ„ç›¸ä¼¼<br>âœ… ä¸­æ–‡æ”¯æŒå¥½ | âŒ æ— éœ€ |
| facebook/opt-125m | 500MB | âš ï¸ é LLaMA æ¶æ„<br>âœ… ä½“ç§¯æœ€å° | âŒ æ— éœ€ |

åˆ›å»º `my_debug_scripts/step1_basic_llama.py`ï¼š

```python
from vllm import LLM, SamplingParams

# ä½¿ç”¨ LLaMA æ¶æ„çš„å°æ¨¡å‹ï¼ˆæ¨èç”¨äºå­¦ä¹ ï¼‰
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"  # å®Œæ•´ LLaMA æ¶æ„ï¼Œæ— éœ€æˆæƒ

# å¦‚æœå·²æœ‰ meta-llama æˆæƒï¼Œå¯ä»¥ä½¿ç”¨ï¼š
# model_name = "meta-llama/Llama-3.2-1B-Instruct"

# æˆ–ä½¿ç”¨æœ¬åœ°ä¸‹è½½çš„æ¨¡å‹ï¼š
# model_name = "/path/to/your/llama-model"

# åˆ›å»ºLLMå®ä¾‹
llm = LLM(
    model=model_name,
    tensor_parallel_size=1,  # å…ˆç”¨å•å¡
    gpu_memory_utilization=0.3,  # æ§åˆ¶KV Cacheé¢„åˆ†é…æ¯”ä¾‹ï¼ˆ0.3=å ç”¨30%æ˜¾å­˜ï¼‰
                                  # å¤šè¿›ç¨‹å¯å…±äº«GPUï¼Œåªè¦æ€»æ˜¾å­˜ä¸è¶…é™
                                  # æƒ…å†µ1: GPUç‹¬å  â†’ å¯è®¾0.9ï¼ˆé«˜ååï¼‰
                                  # æƒ…å†µ2: å¤šè¿›ç¨‹å…±äº« â†’ è®¾0.3-0.5ï¼ˆä¸ºå…¶ä»–è¿›ç¨‹ç•™ç©ºé—´ï¼‰
                                  # æƒ…å†µ3: æ˜¾å­˜ç´§å¼  â†’ é…åˆkv_cache_dtype="int8"é‡åŒ–
    max_model_len=2048,
    trust_remote_code=True
)

# ç®€å•æ¨ç†
prompts = ["Hello, my name is", "The capital of China is"]
sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=50)
outputs = llm.generate(prompts, sampling_params)

for output in outputs:
    print(f"Prompt: {output.prompt}")
    print(f"Output: {output.outputs[0].text}\n")
```

**è¿è¡Œæ–¹å¼ï¼š**

```bash
# æ–¹å¼1: ä½¿ç”¨ VSCode è°ƒè¯•ï¼ˆæ¨èï¼‰
# æŒ‰ F5ï¼Œé€‰æ‹© "ğŸš€ Step1: åŸºç¡€ LLaMA æ¨ç†"

# æ–¹å¼2: å‘½ä»¤è¡Œè¿è¡Œ
python my_debug_scripts/step1_basic_llama.py

# æ–¹å¼3: æŒ‡å®š GPUï¼ˆé¿å…å†²çªï¼‰
CUDA_VISIBLE_DEVICES=1 python my_debug_scripts/step1_basic_llama.py

# æŸ¥çœ‹ GPU å ç”¨æƒ…å†µ
nvidia-smi
```

**è°ƒè¯•ä»»åŠ¡ï¼š**
1. åœ¨ `vllm/__init__.py` çš„ `LLM.__init__()` è®¾ç½®æ–­ç‚¹
2. è·Ÿè¸ªæ¨¡å‹åŠ è½½è¿‡ç¨‹ï¼š`vllm/model_executor/models/llama.py` ä¸­çš„ `LlamaForCausalLM` ç±»
3. ç†è§£å…³é”®ç»„ä»¶åˆå§‹åŒ–é¡ºåº
4. è§‚å¯Ÿ TinyLlama å¦‚ä½•å¤ç”¨æ ‡å‡† LLaMA å®ç°

#### 1.2 æ ¸å¿ƒä»£ç è·Ÿè¸ªè·¯å¾„

**æ¨ç†æµç¨‹å…³é”®æ–‡ä»¶ï¼š**

```
ç”¨æˆ·è°ƒç”¨
  â†“
vllm/__init__.py::LLM.generate()
  â†“
vllm/v1/engine/llm_engine.py::LLMEngine
  â†“
vllm/v1/worker/gpu_worker.py::Worker
  â†“
vllm/model_executor/models/llama.py::LlamaForCausalLM
  â†“
vllm/attention/layer.py::Attention (PagedAttentionæ ¸å¿ƒ)
```

**å­¦ä¹ æ£€æŸ¥ç‚¹ï¼š**
- [ ] ç†è§£ `LLM` ç±»çš„åˆå§‹åŒ–æµç¨‹
- [ ] æ‰¾åˆ°æ¨¡å‹æƒé‡åŠ è½½ä½ç½®ï¼ˆ`model_executor/model_loader/`ï¼‰
- [ ] ç†è§£ `SamplingParams` å¦‚ä½•å½±å“ç”Ÿæˆç­–ç•¥
- [ ] è¿½è¸ªä¸€ä¸ªtokençš„ç”Ÿæˆè¿‡ç¨‹

---

### é˜¶æ®µ2ï¼šæ ¸å¿ƒæŠ€æœ¯æ·±å…¥ï¼ˆ3-4å‘¨ï¼‰

#### 2.1 PagedAttentionæœºåˆ¶

**æ ¸å¿ƒæ–‡ä»¶ï¼š**
- `vllm/attention/layer.py` - Attentionå±‚å®ç°
- `csrc/attention/` - CUDA kernelå®ç°
- `vllm/_custom_ops.py` - Pythonåˆ°C++çš„æ¡¥æ¥

**è°ƒè¯•è„šæœ¬** `my_debug_scripts/step2_paged_attention.py`ï¼š

```python
from vllm import LLM, SamplingParams
import torch

# å¼€å¯è¯¦ç»†æ—¥å¿—
import os
os.environ["VLLM_LOGGING_LEVEL"] = "DEBUG"

llm = LLM(
    model="meta-llama/Llama-3.2-1B-Instruct",
    tensor_parallel_size=1,
    gpu_memory_utilization=0.9,
    block_size=16,  # PagedAttentionçš„blockå¤§å°
    max_num_seqs=8,  # å¹¶å‘å¤„ç†åºåˆ—æ•°
    enable_prefix_caching=True,  # å¼€å¯prefix caching
)

# æµ‹è¯•ä¸åŒé•¿åº¦çš„åºåˆ—
prompts = [
    "Write a short story: " * 10,  # é•¿prompt
    "Hello",  # çŸ­prompt
]

outputs = llm.generate(prompts, SamplingParams(max_tokens=100))
```

**å­¦ä¹ ä»»åŠ¡ï¼š**
1. **ç†è§£PagedAttentionåŸç†ï¼š**
   - åœ¨ `vllm/attention/layer.py::Attention.forward()` è®¾ç½®æ–­ç‚¹
   - è§‚å¯ŸKV cacheå¦‚ä½•åˆ†å—ç®¡ç†
   - ç†è§£ `block_table` çš„ä½œç”¨

2. **è¿½è¸ªCUDA Kernelï¼š**
   - æŸ¥çœ‹ `csrc/attention/attention_kernels.cu`
   - ç†è§£paged_attention_v1/v2çš„åŒºåˆ«

3. **å†…å­˜ç®¡ç†ï¼š**
   - ç ”ç©¶ `vllm/v1/core/scheduler.py` ä¸­çš„è°ƒåº¦é€»è¾‘
   - ç†è§£blockåˆ†é…å’Œå›æ”¶æœºåˆ¶

**æ£€æŸ¥ç‚¹ï¼š**
- [ ] ç”»å‡ºPagedAttentionçš„å†…å­˜å¸ƒå±€å›¾
- [ ] è§£é‡Šä¸ºä»€ä¹ˆPagedAttentionèƒ½æé«˜ååé‡
- [ ] ç†è§£block_sizeå‚æ•°çš„å½±å“

#### 2.2 Continuous Batchingï¼ˆæŒç»­æ‰¹å¤„ç†ï¼‰

**æ ¸å¿ƒæ–‡ä»¶ï¼š**
- `vllm/v1/core/scheduler.py` - è°ƒåº¦å™¨æ ¸å¿ƒ
- `vllm/v1/engine/llm_engine.py` - è¯·æ±‚å¤„ç†

**è°ƒè¯•è„šæœ¬** `my_debug_scripts/step2_continuous_batching.py`ï¼š

```python
import asyncio
from vllm import AsyncLLM, SamplingParams

async def test_continuous_batching():
    llm = AsyncLLM(
        model="meta-llama/Llama-3.2-1B-Instruct",
        tensor_parallel_size=1,
        max_num_seqs=16,  # å¢å¤§å¹¶å‘æ•°
    )
    
    # æ¨¡æ‹Ÿé™†ç»­åˆ°è¾¾çš„è¯·æ±‚
    prompts = [f"Task {i}: " for i in range(20)]
    
    async def generate_with_delay(prompt, delay):
        await asyncio.sleep(delay)
        async for output in llm.generate(prompt, SamplingParams(max_tokens=50)):
            print(f"[{delay}s] {output.outputs[0].text[:50]}...")
    
    # è¯·æ±‚åœ¨ä¸åŒæ—¶é—´åˆ°è¾¾
    tasks = [generate_with_delay(p, i*0.5) for i, p in enumerate(prompts)]
    await asyncio.gather(*tasks)

asyncio.run(test_continuous_batching())
```

**å­¦ä¹ ä»»åŠ¡ï¼š**
1. åœ¨ `scheduler.py::schedule()` è®¾ç½®æ–­ç‚¹
2. è§‚å¯Ÿæ–°è¯·æ±‚å¦‚ä½•æ’å…¥æ­£åœ¨æ‰§è¡Œçš„batch
3. ç†è§£prefillå’Œdecodeé˜¶æ®µçš„åŒºåˆ«

**æ£€æŸ¥ç‚¹ï¼š**
- [ ] è§£é‡Šprefill vs decodeçš„åŒºåˆ«
- [ ] ç†è§£chunked prefillçš„ä½œç”¨
- [ ] ç»˜åˆ¶continuous batchingçš„æ—¶åºå›¾

#### 2.3 å¼ é‡å¹¶è¡Œï¼ˆTensor Parallelismï¼‰

**æ ¸å¿ƒæ–‡ä»¶ï¼š**
- `vllm/distributed/parallel_state.py` - å¹¶è¡ŒçŠ¶æ€ç®¡ç†
- `vllm/model_executor/layers/linear.py` - å¹¶è¡Œçº¿æ€§å±‚
- `vllm/model_executor/models/llama.py` - LLaMAçš„å¹¶è¡Œå®ç°

**è°ƒè¯•è„šæœ¬** `my_debug_scripts/step2_tensor_parallel.py`ï¼š

```python
from vllm import LLM, SamplingParams

# ä½¿ç”¨2å¼ GPUè¿›è¡Œå¼ é‡å¹¶è¡Œ
llm = LLM(
    model="meta-llama/Llama-3.2-3B-Instruct",
    tensor_parallel_size=2,  # 2å¼ å¡TP
    gpu_memory_utilization=0.9,
)

prompts = ["Explain tensor parallelism in deep learning:"]
outputs = llm.generate(prompts, SamplingParams(max_tokens=200))

print(outputs[0].outputs[0].text)
```

**å­¦ä¹ ä»»åŠ¡ï¼š**
1. **ç†è§£å±‚åˆ‡åˆ†ï¼š**
   - åœ¨ `vllm/model_executor/layers/linear.py::ColumnParallelLinear` è®¾ç½®æ–­ç‚¹
   - è§‚å¯Ÿæƒé‡å¦‚ä½•æŒ‰åˆ—åˆ‡åˆ†
   - ç†è§£all-reduceæ“ä½œçš„æ—¶æœº

2. **è¿½è¸ªé€šä¿¡ï¼š**
   - æŸ¥çœ‹ `vllm/distributed/communication_op.py`
   - ç†è§£ä»€ä¹ˆæ—¶å€™éœ€è¦GPUé—´é€šä¿¡

**æ£€æŸ¥ç‚¹ï¼š**
- [ ] ç”»å‡ºLLaMAæ¨¡å‹åœ¨TP=2æ—¶çš„åˆ‡åˆ†æ–¹å¼
- [ ] ç†è§£QKV projectionä¸ºä½•ç”¨ColumnParallel
- [ ] ç†è§£output projectionä¸ºä½•ç”¨RowParallel

#### 2.4 KV Cacheé‡åŒ–

**è°ƒè¯•è„šæœ¬** `my_debug_scripts/step2_kv_quantization.py`ï¼š

```python
from vllm import LLM, SamplingParams

# å¯¹æ¯”FP16 vs INT8 KV cache
configs = [
    ("FP16 KV Cache", {"kv_cache_dtype": "auto"}),
    ("INT8 KV Cache", {"kv_cache_dtype": "int8"}),
]

for name, kwargs in configs:
    llm = LLM(
        model="meta-llama/Llama-3.2-1B-Instruct",
        tensor_parallel_size=1,
        gpu_memory_utilization=0.9,
        **kwargs
    )
    
    prompts = ["Write a story: "] * 10
    outputs = llm.generate(prompts, SamplingParams(max_tokens=100))
    print(f"{name}: Generated {len(outputs)} sequences")
```

**å­¦ä¹ ä»»åŠ¡ï¼š**
1. åœ¨ `vllm/attention/layer.py` ä¸­æ‰¾åˆ°KVé‡åŒ–ä»£ç 
2. ç†è§£é‡åŒ–å¦‚ä½•èŠ‚çœæ˜¾å­˜
3. æµ‹è¯•ä¸åŒé‡åŒ–æ–¹å¼çš„ç²¾åº¦å½±å“

---

### é˜¶æ®µ3ï¼šæ¨¡å‹é€‚é…å®æˆ˜ï¼ˆ2-3å‘¨ï¼‰

#### 3.1 ä»å¤´é€‚é…ä¸€ä¸ªLLaMAå˜ä½“

**ä»»åŠ¡ï¼šä¸ºä¸€ä¸ªæ–°çš„LLaMAå˜ä½“æ·»åŠ æ”¯æŒï¼ˆå¦‚Llama-3.3æˆ–è‡ªå®šä¹‰æ”¹è¿›ï¼‰**

**é€‚é…æ­¥éª¤æ¨¡æ¿ï¼š**

1. **åˆ›å»ºæ¨¡å‹æ–‡ä»¶** `vllm/model_executor/models/my_llama.py`ï¼š

```python
# å‚è€ƒ vllm/model_executor/models/llama.py
from vllm.model_executor.models.llama import (
    LlamaForCausalLM,
    LlamaAttention,
    LlamaMLP,
)

class MyLlamaAttention(LlamaAttention):
    """è‡ªå®šä¹‰Attentionä¿®æ”¹"""
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        # æ·»åŠ è‡ªå®šä¹‰åˆå§‹åŒ–
    
    def forward(self, *args, **kwargs):
        # æ·»åŠ è‡ªå®šä¹‰é€»è¾‘
        return super().forward(*args, **kwargs)

class MyLlamaForCausalLM(LlamaForCausalLM):
    """è‡ªå®šä¹‰LLaMAæ¨¡å‹"""
    pass
```

2. **æ³¨å†Œæ¨¡å‹** - ä¿®æ”¹ `vllm/model_executor/models/__init__.py`ï¼š

```python
"MyLlamaForCausalLM": ("my_llama", "MyLlamaForCausalLM"),
```

3. **æ·»åŠ é…ç½®æ”¯æŒ** - åœ¨ `vllm/transformers_utils/config.py` ä¸­æ³¨å†Œ

4. **ç¼–å†™æµ‹è¯•**ï¼š

```python
# tests/models/test_my_llama.py
import pytest
from vllm import LLM, SamplingParams

def test_my_llama_basic():
    llm = LLM(model="path/to/my-llama", trust_remote_code=True)
    outputs = llm.generate("Hello", SamplingParams(max_tokens=10))
    assert len(outputs) == 1
```

**å®æˆ˜ç»ƒä¹ ï¼š**
- [ ] é€‚é…ä¸€ä¸ªå¸¦æœ‰GQAï¼ˆGrouped Query Attentionï¼‰çš„æ¨¡å‹
- [ ] æ·»åŠ è‡ªå®šä¹‰RoPEï¼ˆRotary Position Embeddingï¼‰å˜ä½“
- [ ] æ”¯æŒä¸åŒçš„MLPæ¿€æ´»å‡½æ•°

#### 3.2 ç†è§£æƒé‡åŠ è½½æœºåˆ¶

**æ ¸å¿ƒæ–‡ä»¶ï¼š**
- `vllm/model_executor/model_loader/loader.py`
- `vllm/model_executor/model_loader/weight_utils.py`

**è°ƒè¯•ä»»åŠ¡ï¼š**
1. åœ¨ `loader.py::load_model()` è®¾ç½®æ–­ç‚¹
2. ç†è§£HuggingFaceæƒé‡åˆ°vLLMçš„æ˜ å°„
3. å­¦ä¹ å¦‚ä½•å¤„ç†æƒé‡æ ¼å¼è½¬æ¢

#### 3.3 æ€§èƒ½ä¼˜åŒ–ä¸Profiling

**Profilingè„šæœ¬** `my_debug_scripts/step3_profiling.py`ï¼š

```python
import torch
from vllm import LLM, SamplingParams
from vllm.profiler import LayerwiseProfiler

# å¯ç”¨profiling
llm = LLM(
    model="meta-llama/Llama-3.2-3B-Instruct",
    tensor_parallel_size=2,
)

# ä½¿ç”¨PyTorch Profiler
with torch.profiler.profile(
    activities=[
        torch.profiler.ProfilerActivity.CPU,
        torch.profiler.ProfilerActivity.CUDA,
    ],
    with_stack=True,
) as prof:
    outputs = llm.generate(
        ["Write a long story: "] * 10,
        SamplingParams(max_tokens=512)
    )

# è¾“å‡ºæ€§èƒ½æŠ¥å‘Š
print(prof.key_averages().table(sort_by="cuda_time_total", row_limit=10))
prof.export_chrome_trace("trace.json")  # åœ¨chrome://tracingæŸ¥çœ‹
```

**Nsys Profilingï¼ˆNVIDIAå·¥å…·ï¼‰ï¼š**

```bash
# ä½¿ç”¨Nsysè¿›è¡Œè¯¦ç»†çš„GPUåˆ†æ
nsys profile -o llama_profile \
    --trace=cuda,nvtx,osrt \
    python my_debug_scripts/step1_basic_llama.py

# æŸ¥çœ‹æŠ¥å‘Š
nsys-ui llama_profile.nsys-rep
```

---

## ä¸‰ã€å…³é”®æŠ€æœ¯æ¸…å•ï¼ˆéœ€è¦æŒæ¡ï¼‰

### 3.1 æ ¸å¿ƒæ¦‚å¿µ
- [x] **PagedAttention**: åˆ†å—KVç¼“å­˜ç®¡ç†
- [x] **Continuous Batching**: åŠ¨æ€æ‰¹å¤„ç†è°ƒåº¦
- [x] **Tensor Parallelism**: è·¨GPUæ¨¡å‹åˆ‡åˆ†
- [x] **Chunked Prefill**: åˆ†å—é¢„å¡«å……
- [ ] **Prefix Caching**: å…¬å…±å‰ç¼€å¤ç”¨
- [ ] **Speculative Decoding**: æ¨æµ‹è§£ç åŠ é€Ÿ

### 3.2 æ¨¡å‹æ¶æ„ç†è§£
- [ ] Transformeræ¶æ„ï¼ˆSelf-Attention, MLPï¼‰
- [ ] RoPEä½ç½®ç¼–ç 
- [ ] GQAï¼ˆGrouped Query Attentionï¼‰
- [ ] RMSNorm vs LayerNorm
- [ ] SwiGLUæ¿€æ´»å‡½æ•°

### 3.3 ç³»ç»Ÿä¼˜åŒ–
- [ ] CUDA Kernelç¼–å†™åŸºç¡€
- [ ] FP8/INT8é‡åŒ–
- [ ] FlashAttentioné›†æˆ
- [ ] å†…å­˜æ± ç®¡ç†
- [ ] é€šä¿¡ä¼˜åŒ–ï¼ˆNCCLï¼‰

---

## å››ã€è°ƒè¯•æŠ€å·§ä¸å·¥å…·

### 4.1 è°ƒè¯•å‘½ä»¤

```bash
# 1. å•æ­¥è°ƒè¯•
python -m pdb my_debug_scripts/step1_basic_llama.py

# 2. ä½¿ç”¨ipdbï¼ˆå¢å¼ºç‰ˆpdbï¼‰
import ipdb; ipdb.set_trace()  # åœ¨ä»£ç ä¸­æ’å…¥

# 3. æŸ¥çœ‹GPUå†…å­˜
nvitop  # å®æ—¶ç›‘æ§

# 4. ç¯å¢ƒå˜é‡è°ƒè¯•
export VLLM_LOGGING_LEVEL=DEBUG
export VLLM_TRACE_FUNCTION=1  # å‡½æ•°è°ƒç”¨è¿½è¸ª
export CUDA_LAUNCH_BLOCKING=1  # åŒæ­¥CUDAè°ƒç”¨ï¼Œä¾¿äºè°ƒè¯•

# 5. åªä½¿ç”¨éƒ¨åˆ†GPU
export CUDA_VISIBLE_DEVICES=0,1
```

### 4.2 å¸¸ç”¨æ–­ç‚¹ä½ç½®

```python
# æ¨ç†å…¥å£
vllm/__init__.py::LLM.generate()

# æ¨¡å‹å‰å‘ä¼ æ’­
vllm/model_executor/models/llama.py::LlamaForCausalLM.forward()

# Attentionè®¡ç®—
vllm/attention/layer.py::Attention.forward()

# è°ƒåº¦å™¨
vllm/v1/core/scheduler.py::Scheduler.schedule()

# æƒé‡åŠ è½½
vllm/model_executor/model_loader/loader.py::load_model()
```

### 4.3 æ—¥å¿—åˆ†æ

vLLMçš„æ—¥å¿—åŒ…å«å¤§é‡è°ƒè¯•ä¿¡æ¯ï¼š

```python
# åœ¨ä»£ç ä¸­æ·»åŠ è¯¦ç»†æ—¥å¿—
from vllm.logger import init_logger
logger = init_logger(__name__)

logger.debug("Detailed debug info")
logger.info("Important info")
logger.warning("Warning message")
```

---

## äº”ã€å­¦ä¹ èµ„æº

### 5.1 å¿…è¯»è®ºæ–‡
1. **vLLMåŸè®ºæ–‡**: "Efficient Memory Management for Large Language Model Serving with PagedAttention"
2. **FlashAttention**: "FlashAttention: Fast and Memory-Efficient Exact Attention"
3. **Tensor Parallelism**: "Megatron-LM: Training Multi-Billion Parameter Language Models"

### 5.2 ä»£ç å¯¼è¯»é¡ºåº

```
ç¬¬ä¸€å‘¨ï¼š
â”œâ”€â”€ vllm/__init__.py                 # å…¥å£API
â”œâ”€â”€ examples/offline_inference/basic/basic.py  # ç¤ºä¾‹
â””â”€â”€ vllm/model_executor/models/llama.py       # LLaMAå®ç°

ç¬¬äºŒå‘¨ï¼š
â”œâ”€â”€ vllm/attention/layer.py          # PagedAttention
â”œâ”€â”€ vllm/v1/engine/llm_engine.py    # å¼•æ“
â””â”€â”€ vllm/v1/core/scheduler.py        # è°ƒåº¦å™¨

ç¬¬ä¸‰å‘¨ï¼š
â”œâ”€â”€ vllm/distributed/                # åˆ†å¸ƒå¼
â”œâ”€â”€ vllm/model_executor/layers/      # å¹¶è¡Œå±‚
â””â”€â”€ csrc/                            # CUDA kernels

ç¬¬å››å‘¨ï¼š
â”œâ”€â”€ vllm/model_executor/model_loader/ # æƒé‡åŠ è½½
â”œâ”€â”€ vllm/config/                     # é…ç½®ç³»ç»Ÿ
â””â”€â”€ tests/models/                    # æµ‹è¯•æ ·ä¾‹
```

### 5.3 ç¤¾åŒºèµ„æº
- **å®˜æ–¹æ–‡æ¡£**: https://docs.vllm.ai
- **GitHub Issues**: æŸ¥çœ‹å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
- **Discord/Slack**: åŠ å…¥vLLMç¤¾åŒºè®¨è®º

---

## å…­ã€è¿›åº¦è·Ÿè¸ªæ¸…å•

### Week 1-2: åŸºç¡€å…¥é—¨
- [ ] ç¯å¢ƒæ­å»ºå®Œæˆï¼ˆuv venv + python3-devï¼‰
- [ ] é€‰æ‹©åˆé€‚çš„å­¦ä¹ æ¨¡å‹ï¼ˆæ¨è TinyLlamaï¼‰
- [ ] é…ç½® VSCode è°ƒè¯•ç¯å¢ƒï¼ˆ.vscode/launch.jsonï¼‰
- [ ] è¿è¡Œç¬¬ä¸€ä¸ª LLaMA æ¨ç†ç¤ºä¾‹
- [ ] ç†è§£ LLM ç±»çš„åˆå§‹åŒ–æµç¨‹
- [ ] è¿½è¸ªå®Œæ•´çš„ token ç”Ÿæˆè¿‡ç¨‹
- [ ] é˜…è¯» `llama.py` æ ¸å¿ƒä»£ç ï¼Œç†è§£æ¶æ„å¯¹åº”å…³ç³»

### Week 3-4: PagedAttention
- [ ] ç†è§£PagedAttentionåŸç†
- [ ] è°ƒè¯•KV cacheç®¡ç†
- [ ] ç»˜åˆ¶å†…å­˜å¸ƒå±€å›¾
- [ ] åˆ†æblock_sizeçš„å½±å“

### Week 5-6: Continuous Batching
- [ ] ç†è§£è°ƒåº¦å™¨å·¥ä½œåŸç†
- [ ] åŒºåˆ†prefillå’Œdecode
- [ ] æµ‹è¯•å¹¶å‘è¯·æ±‚å¤„ç†
- [ ] ç†è§£chunked prefill

### Week 7-8: åˆ†å¸ƒå¼æ¨ç†
- [ ] ç†è§£Tensor Parallelism
- [ ] è°ƒè¯•2å¡TPæ¨ç†
- [ ] åˆ†æé€šä¿¡å¼€é”€
- [ ] æµ‹è¯•4å¡TPï¼ˆåˆ©ç”¨ä½ çš„4å¼ 4090ï¼‰

### Week 9-10: æ¨¡å‹é€‚é…
- [ ] é˜…è¯»æ¨¡å‹æ³¨å†Œæœºåˆ¶
- [ ] é€‚é…ä¸€ä¸ªLLaMAå˜ä½“
- [ ] æ·»åŠ è‡ªå®šä¹‰å±‚
- [ ] ç¼–å†™å•å…ƒæµ‹è¯•

### Week 11-12: ä¼˜åŒ–ä¸è°ƒä¼˜
- [ ] ä½¿ç”¨Profileråˆ†ææ€§èƒ½
- [ ] ç†è§£CUDA kernel
- [ ] æµ‹è¯•ä¸åŒé‡åŒ–æ–¹æ¡ˆ
- [ ] ä¼˜åŒ–å†…å­˜å ç”¨

---

## ä¸ƒã€å®æˆ˜é¡¹ç›®å»ºè®®

å®Œæˆå­¦ä¹ åï¼ŒæŒ‘æˆ˜ä»¥ä¸‹é¡¹ç›®æ¥å·©å›ºæŠ€èƒ½ï¼š

1. **é¡¹ç›®1**: ä¸ºLlama-3.3æ·»åŠ å®Œæ•´æ”¯æŒï¼ˆå¦‚æœvLLMè¿˜æœªæ”¯æŒï¼‰
2. **é¡¹ç›®2**: å®ç°è‡ªå®šä¹‰çš„Attentionå˜ä½“ï¼ˆå¦‚Linear Attentionï¼‰
3. **é¡¹ç›®3**: ä¼˜åŒ–å°batchæ¨ç†çš„latency
4. **é¡¹ç›®4**: ä¸ºç‰¹å®šç¡¬ä»¶ï¼ˆå¦‚4090ï¼‰ä¼˜åŒ–kernelå‚æ•°

---

## å…«ã€å¸¸è§é—®é¢˜é€ŸæŸ¥

### Q1: æ˜¾å­˜æº¢å‡º (OOM) æˆ–ä¸å…¶ä»–è¿›ç¨‹å†²çª

**æ˜¾å­˜å ç”¨è®¡ç®—å…¬å¼ï¼š**
```
æ€»æ˜¾å­˜ = æ¨¡å‹æƒé‡ + KV Cache + æ¿€æ´»å€¼ + æ¡†æ¶å¼€é”€

1. æ¨¡å‹æƒé‡ï¼ˆå›ºå®šï¼‰
   = å‚æ•°é‡ Ã— ç²¾åº¦å­—èŠ‚æ•°
   TinyLlama: 1.1B Ã— 2 bytes(FP16) = 2.2GB

2. KV Cacheï¼ˆå¯è°ƒï¼Œæœ€å¤§å¤´ï¼‰
   ã€å…¬å¼è¯¦è§£ã€‘æ¯å±‚æ¯ä¸ªtokençš„KVå­˜å‚¨ï¼š
   å•tokenå•å±‚ = 2(K+V) Ã— num_kv_heads Ã— head_dim Ã— ç²¾åº¦(å­—èŠ‚)
   
   å‚æ•°å«ä¹‰ï¼š
   - 2(K+V): Attentionéœ€è¦ç¼“å­˜Keyå’ŒValueä¸¤ä¸ªçŸ©é˜µ
   - num_kv_heads: KVçš„æ³¨æ„åŠ›å¤´æ•°é‡
     * MHA(å¤šå¤´æ³¨æ„åŠ›): num_kv_heads = num_q_heads (å¦‚32å¤´)
     * GQA(åˆ†ç»„æŸ¥è¯¢): num_kv_heads < num_q_heads (å¦‚4å¤´KVå¯¹åº”32å¤´Q)
     * MQA(å¤šæŸ¥è¯¢): num_kv_heads = 1 (æè‡´æ˜¾å­˜ä¼˜åŒ–)
   - head_dim: æ¯ä¸ªå¤´çš„ç»´åº¦ (é€šå¸¸64æˆ–128)
     è®¡ç®—: hidden_size / num_q_heads
     å¦‚TinyLlama: 2048 / 32 = 64
   - ç²¾åº¦: æ•°æ®ç±»å‹å­—èŠ‚æ•°
     * FP16/BF16: 2å­—èŠ‚
     * INT8: 1å­—èŠ‚ (é‡åŒ–å)
     * FP32: 4å­—èŠ‚
   
   ã€TinyLlamaç¤ºä¾‹ã€‘
   é…ç½®: 32å±‚, 32ä¸ªQå¤´, 4ä¸ªKVå¤´(GQA), head_dim=64, FP16
   å•tokenå•å±‚ = 2 Ã— 4 Ã— 64 Ã— 2 = 1024å­—èŠ‚ = 1KB
   æ€»KVé¢„ç•™ = num_layers Ã— å•å±‚KV Ã— max_model_len Ã— max_num_seqs
   
   å‚æ•°è¯¦è§£ï¼š
   - num_layers (32å±‚): Transformerè§£ç å™¨å±‚æ•°
     æ¯å±‚éƒ½æœ‰ç‹¬ç«‹çš„Attentionï¼Œéœ€è¦ç‹¬ç«‹çš„KV Cache
     å¯æŸ¥çœ‹config.json: "num_hidden_layers": 32
   
   - max_model_len (2048): å•ä¸ªåºåˆ—çš„æœ€å¤§tokené•¿åº¦
     = prompté•¿åº¦ + ç”Ÿæˆçš„outputé•¿åº¦
     
     â“ Tokenæ˜¯ä»€ä¹ˆï¼Ÿ
     Tokenæ˜¯æ¨¡å‹çš„åŸºæœ¬å¤„ç†å•å…ƒï¼Œç”±Tokenizerç®—æ³•åˆ‡åˆ†ï¼ˆä¸æ˜¯ç®€å•çš„"è¯"ï¼‰
     
     è‹±æ–‡Tokenizationï¼ˆæ¥è¿‘å•è¯ï¼Œä½†ä¸å®Œå…¨æ˜¯ï¼‰:
     * å¸¸è§è¯: "hello" = 1 token âœ“
     * é•¿è¯æ‹†åˆ†: "running" = "run" + "ning" = 2 tokens
     * ç½•è§è¯: "ChatGPT" å¯èƒ½è¢«æ‹†æˆ "Chat" + "G" + "PT"
     * æ ‡ç‚¹ç©ºæ ¼: ", " "." ä¹Ÿæ˜¯token
     * æ¢ç®—: ~1ä¸ªå•è¯ â‰ˆ 1.3 tokensï¼ˆå¹³å‡ï¼‰
     
     ä¸­æ–‡Tokenizationï¼ˆâŒ ä¸æ˜¯æŒ‰è¯è¯­åˆ‡åˆ†ï¼ï¼‰:
     * LLaMAç”¨å­—èŠ‚çº§BPEï¼Œæœªå¯¹ä¸­æ–‡ä¼˜åŒ–
     * æ¯ä¸ªæ±‰å­—çš„UTF-8ç¼–ç ï¼ˆ3å­—èŠ‚ï¼‰è¢«æ‹†æˆå¤šä¸ªtokenç‰‡æ®µ
     * "ä½ å¥½"(2å­—) â‰ˆ 4-6 tokensï¼ˆä¸æ˜¯1ä¸ªè¯=1ä¸ªtokenï¼‰
     * "äººå·¥æ™ºèƒ½"(4å­—) â‰ˆ 8-12 tokens
     * æ¢ç®—: ~1ä¸ªæ±‰å­— â‰ˆ 1.5-2 tokens
     
     âœ… å®é™…æµ‹è¯•æ–¹æ³•ï¼š
     ```python
     from transformers import AutoTokenizer
     tokenizer = AutoTokenizer.from_pretrained("TinyLlama/TinyLlama-1.1B-Chat-v1.0")
     text = "Hello world, ä½ å¥½ä¸–ç•Œ"
     tokens = tokenizer.encode(text)
     print(f"æ–‡æœ¬: {text}")
     print(f"Tokenæ•°: {len(tokens)}")  # å®é™…tokenæ•°é‡
     print(f"Tokens: {tokens}")
     ```
     
     ä½¿ç”¨ç¤ºä¾‹:
     - ç”¨æˆ·è¾“å…¥: "å†™ä¸€ç¯‡å…³äºAIçš„æ–‡ç« "(~15 tokens)
     - æ¨¡å‹ç”Ÿæˆ: 500å­—è‹±æ–‡(~125 tokens) æˆ– 500æ±‰å­—(~750 tokens)
     - æ€»è®¡: 140 æˆ– 765 tokens < 2048 âœ… å¯ä»¥å¤„ç†
     
     âš ï¸ è®¾å¤ªå¤§æµªè´¹æ˜¾å­˜ï¼Œè®¾å¤ªå°ä¼šæˆªæ–­é•¿æ–‡æœ¬
   
   - max_num_seqs (256): æœ€å¤§å¹¶å‘åºåˆ—æ•° = batch size
     åŒæ—¶å¤„ç†å¤šå°‘ä¸ªä¸åŒçš„è¯·æ±‚
     ä¾‹: 256ä¸ªç”¨æˆ·åŒæ—¶å‘é€è¯·æ±‚ï¼ŒvLLMå¯ä»¥ä¸€èµ·å¤„ç†
     âš ï¸ è¶Šå¤§ååé‡è¶Šé«˜ï¼Œä½†æ˜¾å­˜éœ€æ±‚æˆå€å¢åŠ ï¼
   
   å®é™…è®¡ç®—:
   KV Cache = 32å±‚ Ã— 1KB Ã— 2048 tokens Ã— 256å¹¶å‘ â‰ˆ 16GB (ç†è®ºæœ€å¤§)
   
   âš ï¸ gpu_memory_utilization å·¥ä½œæœºåˆ¶ï¼š
   æ˜¾å­˜åˆ†é…é¡ºåºï¼š
   1. å…ˆåŠ è½½æ¨¡å‹æƒé‡ï¼ˆ2.2GBï¼‰         â†’ å ç”¨GPUæ˜¾å­˜
   2. æ¡†æ¶åˆå§‹åŒ–å¼€é”€ï¼ˆ~1GBï¼‰          â†’ å ç”¨GPUæ˜¾å­˜  
   3. è®¡ç®—å‰©ä½™å¯ç”¨æ˜¾å­˜: 24GB - 3.2GB = 20.8GB
   4. KV Cacheé¢„åˆ†é…: 20.8GB Ã— 0.3 = 6.24GB â†’ å ç”¨GPUæ˜¾å­˜
   5. æ¿€æ´»å€¼åŠ¨æ€åˆ†é…ï¼ˆå‡ ç™¾MBï¼‰        â†’ å ç”¨GPUæ˜¾å­˜
   
   âœ… æ‰€æœ‰å†…å­˜éƒ½åœ¨GPUä¸Šï¼gpu_memory_utilizationåªæ§åˆ¶KV Cache
   è®¾ä¸º0.3: é™åˆ¶KV Cacheæœ€å¤šå ç”¨(å‰©ä½™æ˜¾å­˜Ã—30%)
   è®¾ä¸º0.9: KV Cacheå¯ç”¨(å‰©ä½™æ˜¾å­˜Ã—90%)ï¼Œä¸ºæ¿€æ´»å€¼ç•™10%

3. æ¿€æ´»å€¼ï¼ˆåŠ¨æ€ï¼‰
   = batch_size Ã— seq_len Ã— hidden_size Ã— å±‚æ•° Ã— ç²¾åº¦
   é€šå¸¸å‡ ç™¾MBåˆ°å‡ GB

4. æ¡†æ¶å¼€é”€: PyTorch/CUDAå ç”¨çº¦1-2GB
```

**å®é™…æ˜¾å­˜éœ€æ±‚ç¤ºä¾‹ï¼ˆTinyLlamaï¼‰ï¼š**
| é…ç½® | æ¨¡å‹æƒé‡ | KV Cacheé¢„åˆ†é… | å…¶ä»– | æ€»è®¡ |
|------|----------|----------------|------|------|
| `gpu_memory_utilization=0.9` | 2.2GB | 21.6GB | 1GB | ~24.8GB âŒ å•è¿›ç¨‹ |
| `gpu_memory_utilization=0.3` | 2.2GB | 7.2GB | 1GB | ~10.4GB âœ… å¯å…±äº« |
| `gpu_memory_utilization=0.3`<br>`+ kv_cache_dtype="int8"` | 2.2GB | 3.6GB | 1GB | ~6.8GB âœ… æ›´çœ |

```bash
# æŸ¥çœ‹ GPU ä½¿ç”¨æƒ…å†µ
nvidia-smi
ps aux | grep -E "python.*vllm|vllm.*serve" | grep -v grep

# æŒ‡å®šä½¿ç”¨å…¶ä»– GPU
export CUDA_VISIBLE_DEVICES=1  # ä½¿ç”¨ GPU 1
python my_debug_scripts/step1_basic_llama.py
```

```python
# æ–¹æ¡ˆ1: é™ä½KV Cacheé¢„åˆ†é…
llm = LLM(model=..., gpu_memory_utilization=0.3)  # 7.2GB KV Cache

# æ–¹æ¡ˆ2: å‡å°‘å¹¶å‘åºåˆ—æ•°
llm = LLM(model=..., max_num_seqs=64)  # ä»256é™åˆ°64

# æ–¹æ¡ˆ3: ç¼©çŸ­æœ€å¤§åºåˆ—é•¿åº¦
llm = LLM(model=..., max_model_len=1024)  # ä»2048é™åˆ°1024

# æ–¹æ¡ˆ4: KV Cacheé‡åŒ–ï¼ˆæ˜¾å­˜å‡åŠï¼‰
llm = LLM(model=..., kv_cache_dtype="int8")  # FP16â†’INT8
```

### Q2: å¤šå¡ä¸å‡è¡¡
```bash
# æ£€æŸ¥NCCL
export NCCL_DEBUG=INFO

# ä½¿ç”¨nvlink topology
nvidia-smi topo -m
```

### Q3: æ¨ç†é€Ÿåº¦æ…¢
```python
# å¯ç”¨prefix caching
llm = LLM(model=..., enable_prefix_caching=True)

# å¢å¤§batch size
outputs = llm.generate(prompts * 10, ...)  # æ›´å¤§batch

# ä½¿ç”¨æ›´å¤§block_size
llm = LLM(model=..., block_size=32)  # é»˜è®¤16
```

---

## ä¹ã€æ€»ç»“

è¿™ä¸ªå­¦ä¹ è®¡åˆ’çš„æ ¸å¿ƒæ€è·¯æ˜¯ï¼š
1. **ç”±æµ…å…¥æ·±**ï¼šä»ç®€å•ç¤ºä¾‹åˆ°å¤æ‚æœºåˆ¶
2. **ç†è®ºç»“åˆå®è·µ**ï¼šæ¯ä¸ªæ¦‚å¿µéƒ½æœ‰è°ƒè¯•è„šæœ¬éªŒè¯
3. **ä»¥ç»ˆä¸ºå§‹**ï¼šç›®æ ‡æ˜¯é€‚é…æ–°æ¨¡å‹ï¼Œæ‰€ä»¥é‡ç‚¹åœ¨æ¶æ„ç†è§£

**å­¦ä¹ èŠ‚å¥å»ºè®®**ï¼š
- æ¯å¤©2-3å°æ—¶æ·±åº¦å­¦ä¹ 
- ä¸€å‘¨å®Œæˆä¸€ä¸ªé˜¶æ®µ
- è¾¹å­¦è¾¹è®°å½•ç¬”è®°å’Œè¸©å‘ç»éªŒ

**æˆåŠŸæ ‡å‡†**ï¼š
- èƒ½ç‹¬ç«‹ä¸ºæ–°çš„LLaMAå˜ä½“æ·»åŠ vLLMæ”¯æŒ
- ç†è§£vLLMçš„5å¤§æ ¸å¿ƒæŠ€æœ¯ï¼ˆPagedAttention, Continuous Batching, TP, Chunked Prefill, KV Quantizationï¼‰
- èƒ½åˆ†æå’Œè§£å†³å¸¸è§çš„æ€§èƒ½/æ˜¾å­˜é—®é¢˜

ç¥å­¦ä¹ é¡ºåˆ©ï¼æœ‰ä»»ä½•é—®é¢˜éšæ—¶åœ¨ä»£ç ä¸­è®¾æ–­ç‚¹æ·±å…¥æ¢ç´¢ã€‚
